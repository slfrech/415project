{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUkzQQVZ5ZH1"
      },
      "outputs": [],
      "source": [
        "# import needed libraries\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE8iQCl2Q6l8"
      },
      "outputs": [],
      "source": [
        "# read the hotel reservation data in\n",
        "\n",
        "dataset = pd.read_csv('booking.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owP4sB-jiwkg"
      },
      "source": [
        "### Clean dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtYMweAcmzci"
      },
      "source": [
        "Drop Booking_ID variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IH_sqYz8m1o3"
      },
      "outputs": [],
      "source": [
        "# drop the booking_id variable - doesn't offer any valuable insight\n",
        "dataset = dataset.drop('Booking_ID', axis=1)\n",
        "\n",
        "# view to make sure that the variable has been dropped\n",
        "print(dataset.shape)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mPm9Jxoi5tE"
      },
      "source": [
        "Check for missing/null values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYVAc6Pii8Ep"
      },
      "outputs": [],
      "source": [
        "# sum up the null values for each variable in the dataset\n",
        "null_vals = dataset.isnull().sum()\n",
        "# view total number of null values for each variable\n",
        "null_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR4M1WRlmQbY"
      },
      "source": [
        "Encode categorical variables with numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppfOBzP_mOVS"
      },
      "outputs": [],
      "source": [
        "# determine data type of each variable\n",
        "data_types = dataset.dtypes\n",
        "\n",
        "data_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTtcZDQtmh3Q"
      },
      "outputs": [],
      "source": [
        "# view unique values of categorical variables in dataset\n",
        "# categorical variables include: type of meal, room type, market segment type, booking status\n",
        "print(\"Unique values for type of meal:\", dataset['type of meal'].unique())\n",
        "\n",
        "print(\"Unique values for room type:\", dataset['room type'].unique())\n",
        "\n",
        "print(\"Unique values for market segment type:\", dataset['market segment type'].unique())\n",
        "\n",
        "print(\"Unique values for booking status:\", dataset['booking status'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_4zi9woEXH"
      },
      "outputs": [],
      "source": [
        "# use a numerical coding method to encode the categorical variables\n",
        "dataset['type of meal'] = dataset['type of meal'].replace({'Not Selected': 0, 'Meal Plan 1': 1, 'Meal Plan 2': 2, 'Meal Plan 3': 3});\n",
        "dataset['room type'] = dataset['room type'].replace({'Room_Type 1': 1, 'Room_Type 2': 2, 'Room_Type 3': 3, 'Room_Type 4': 4, 'Room_Type 5': 5,\n",
        "                                                     'Room_Type 6': 6, 'Room_Type 7': 7});\n",
        "dataset['market segment type'] = dataset['market segment type'].replace({'Offline': 0, 'Online': 1, 'Corporate': 2, 'Aviation': 3, 'Complementary': 4});\n",
        "dataset['booking status'] = dataset['booking status'].replace({'Not_Canceled': 0, 'Canceled': 1});\n",
        "\n",
        "# view dataset to make sure changes were successful\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3U7N-wqth8T"
      },
      "source": [
        "Split date into separate month/day/year variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkpM5XQat2jk"
      },
      "outputs": [],
      "source": [
        "# drop any dates that are not in the correct format (at least one uses '-' instead of '/')\n",
        "dataset = dataset[~dataset[\"date of reservation\"].str.contains(\"-\")]\n",
        "\n",
        "# access date of registration variable and split based on '/' - make 3 new variables for month/day/year\n",
        "dataset[[\"month of reservation\", \"day of reservation\", \"year of reservation\"]] = dataset[\"date of reservation\"].str.split(\"/\", expand = True)\n",
        "\n",
        "# convert the data types to int types\n",
        "dataset['month of reservation'] = dataset['month of reservation'].astype(int)\n",
        "dataset['day of reservation'] = dataset['day of reservation'].astype(int)\n",
        "dataset['year of reservation'] = dataset['year of reservation'].astype(int)\n",
        "\n",
        "# drop original date of reservation variable\n",
        "dataset = dataset.drop('date of reservation', axis=1)\n",
        "\n",
        "# view dataset to make sure changes were successful\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSHSZhsEvfkq"
      },
      "outputs": [],
      "source": [
        "# look at data types one more time to ensure everything is a numerical type\n",
        "data_types = dataset.dtypes\n",
        "\n",
        "data_types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3r984LzNct"
      },
      "source": [
        "Check for outliers in the dataset - No longer using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFRG9NnD6GbO"
      },
      "outputs": [],
      "source": [
        "# # use boxplot to visualize potential outliers\n",
        "# # only visualize those that were numerical data types originally\n",
        "\n",
        "# sns.boxplot(data=dataset[['number of adults', 'number of children', 'number of weekend nights', 'number of week nights',\n",
        "#                           'car parking space', 'lead time', 'repeated', 'P-C', 'P-not-C', 'average price', 'special requests']])\n",
        "# plt.title(\"Boxplots to Show Outliers for Each Variable\")\n",
        "# plt.xlabel(\"Variables\")\n",
        "# plt.ylabel(\"Values\")\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.show()\n",
        "\n",
        "# fig, axes = plt.subplots(1, 11, figsize=(20, 5))\n",
        "\n",
        "# axes[0].hist(dataset['lead time'], bins=20, color='skyblue', edgecolor='black')\n",
        "\n",
        "# axes[1].hist(dataset['average price'], bins=20, color='lightgreen', edgecolor='black')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSlZd6bVj5M1"
      },
      "outputs": [],
      "source": [
        "# # look for outliers mathematically - can a create a function that will find and drop outliers based on IQR\n",
        "# def find_and_drop_outliers_IQR(dataset, variable_list):\n",
        "\n",
        "#   # loop through variable list\n",
        "#   for variable in variable_list:\n",
        "#     # get the variable column\n",
        "#     variable_col = dataset[variable]\n",
        "#     # find first quartile\n",
        "#     Q1 = variable_col.quantile(0.25)\n",
        "#     # find third quartile\n",
        "#     Q3 = variable_col.quantile(0.75)\n",
        "#     # compute interquartile range (IQR)\n",
        "#     IQR = Q3 - Q1\n",
        "\n",
        "#     # compute lower bound (anything below this value is an outlier)\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     # compute upper bound (anything above this value is an outlier)\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "#     # filter out rows where the column value is outside the bounds - so only keep what is in bounds\n",
        "#     dataset = dataset[(variable_col >= lower_bound) & (variable_col <= upper_bound)]\n",
        "\n",
        "\n",
        "#   return dataset\n",
        "\n",
        "# # define variable list\n",
        "# variable_list = ['lead time', 'average price']\n",
        "# # call the function on each variable that we want to look for outliers\n",
        "# data_clean = find_and_drop_outliers_IQR(dataset, variable_list)\n",
        "\n",
        "# data_clean.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9r5Jdw_Blbk"
      },
      "outputs": [],
      "source": [
        "# # make a new plot to ensure that any outliers have been removed\n",
        "\n",
        "# sns.boxplot(data=data_clean[['number of adults', 'number of children', 'number of weekend nights', 'number of week nights',\n",
        "#                           'car parking space', 'lead time', 'repeated', 'P-C', 'P-not-C', 'average price', 'special requests']])\n",
        "# plt.title(\"Boxplots With Outliers Removed for Each Variable\")\n",
        "# plt.xlabel(\"Variables\")\n",
        "# plt.ylabel(\"Values\")\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# fig, axes = plt.subplots(1, 11, figsize=(20, 5))\n",
        "\n",
        "# axes[0].hist(data_clean['lead time'], bins=20, color='skyblue', edgecolor='black')\n",
        "\n",
        "# axes[1].hist(data_clean['average price'], bins=20, color='lightgreen', edgecolor='black')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7O1D6u3FwfD"
      },
      "outputs": [],
      "source": [
        "# assign dataset to a variable called data_clean to avoid changing variable name throughout code\n",
        "# due to decision not to remove outiers\n",
        "data_clean = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnnWSeik5Jyy"
      },
      "source": [
        "### Exploratory analysis of dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6jm_6OA_FTV"
      },
      "source": [
        "Check the counts for each class of the response variable (booking status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wDHdqKk_DlK"
      },
      "outputs": [],
      "source": [
        "print(\"Counts for each class of 'booking status':\")\n",
        "print(data_clean['booking status'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PeRYstVT4mBQ"
      },
      "outputs": [],
      "source": [
        "# show relationship between all variables\n",
        "\n",
        "sns.pairplot(data_clean)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLY8tYRixnVq"
      },
      "outputs": [],
      "source": [
        "# create correlation matrix between all variables to look at relationships of variables with response variable (booking status)\n",
        "correlation_matrix = data_clean.corr()\n",
        "\n",
        "# extract correlation between variables and the response variable\n",
        "correlation_booking_status = correlation_matrix['booking status']\n",
        "\n",
        "# show correlations\n",
        "correlation_booking_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuyJZnS0yYUS"
      },
      "source": [
        "Compare relationship between highest correlation predictors and response variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OssyCmiVwfGx"
      },
      "outputs": [],
      "source": [
        "# create box plot for lead time and booking status (correlation coeff = 0.4106)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x = 'booking status', y = 'lead time', data = data_clean)\n",
        "\n",
        "# add title\n",
        "plt.title('Lead Time by Booking Status')\n",
        "# add x and y labels\n",
        "plt.xlabel('Booking Status')\n",
        "plt.ylabel('# Days Between Booking and Arrival Date')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZBKz7bf4X9u"
      },
      "outputs": [],
      "source": [
        "# create box plot for special requests and booking status (correlation coeff = -0.245)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x = 'booking status', y = 'special requests', data = data_clean)\n",
        "\n",
        "# add title\n",
        "plt.title('Special Requests by Booking Status')\n",
        "# add x and y labels\n",
        "plt.xlabel('Booking Status')\n",
        "plt.ylabel('# Special Requests Made by Guest')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUoAoTqj5WP8"
      },
      "outputs": [],
      "source": [
        "# create box plot for average price and booking status (correlation coeff = -0.245)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x = 'booking status', y = 'average price', data = data_clean)\n",
        "\n",
        "# add title\n",
        "plt.title('Average Booking Price by Booking Status')\n",
        "# add x and y labels\n",
        "plt.xlabel('Booking Status')\n",
        "plt.ylabel('Average Price Associated with Booking')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Xmfc61xHOL"
      },
      "source": [
        "### Split the data into training and testing set (70/30 split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uJtj3_lxGm1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "# determine how many samples make up 30$ of the dataset\n",
        "test_n = len(data_clean) * 0.3\n",
        "# round the number down\n",
        "test_n = math.floor(test_n)\n",
        "# create a dataframe that includes all columns except 'booking status' (so only predictors)\n",
        "predictors = data_clean.drop(columns=['booking status'])\n",
        "# create a dataframe that includes only the 'booking status' column\n",
        "booking_status = data_clean['booking status']\n",
        "# split the dataset using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors,\n",
        "                                                    booking_status,\n",
        "                                                    test_size = test_n,\n",
        "                                                    random_state = 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8PFXLX80odV"
      },
      "source": [
        "Scaled data - Scale all predictors to have mean 0 and variance 1 - Use for models where performance benefits from scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7BsFFb00n3D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on X_train and scale the features\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale X_test using the same scaler\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrames with the original columns and reset indices\n",
        "X_train_scaled = pd.DataFrame(scaled_X_train, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(scaled_X_test, columns=X_test.columns, index=X_test.index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6oy6kBC2XC5"
      },
      "source": [
        "### Implement baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0sZASRuni_p"
      },
      "source": [
        "***Logistic Regression***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luL15aI38RP0"
      },
      "source": [
        "Implement function that performs forward stepwise logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEppQqQJeSnK"
      },
      "source": [
        "1. Use all predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoSo5MoYeZQG"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "# fit a model using all predictors\n",
        "model = sm.Logit(y_train, sm.add_constant(X_train)).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9hCLbsjemGC"
      },
      "source": [
        "Test the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t2lMEcoenFB"
      },
      "outputs": [],
      "source": [
        "# create design matrix for test set\n",
        "X_test_all_design = sm.add_constant(X_test)\n",
        "# get predicted probabilities from the model\n",
        "predicted_probs_all = model.predict(X_test_all_design)\n",
        "# convert probabilities to predicted classes\n",
        "predicted_classes_all = (predicted_probs_all > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegAvyM2ewEj"
      },
      "source": [
        "Evaluate the model using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRthwDeleygi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_classes_all))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, predicted_classes_all))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, predicted_classes_all))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, predicted_classes_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycbc5sl4e4_8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, predicted_classes_all)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Logistic Regression Model Confusion Matrix - All Predictors')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UUL-btJDK0o"
      },
      "source": [
        "2. Use AIC as criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8LCI4Hjr2acE"
      },
      "outputs": [],
      "source": [
        "# write a function that performs forward stepwise logistic regression using AIC criterion\n",
        "def forward_stepwise_logit_AIC(X_train, y_train):\n",
        "  # keep track of predictor variables that are still available to be added to the model\n",
        "  remaining_predictors = list(X_train.columns)\n",
        "  # list of selected predictor variables for the model\n",
        "  selected_predictors = []\n",
        "  # create variables for criterion that we want to minimize - initially set to infinity\n",
        "  current_best_aic, new_best_aic = float('inf'), float('inf')\n",
        "\n",
        "  # loop (at max) until there are no more predictors left to be added\n",
        "  while remaining_predictors:\n",
        "\n",
        "    aic_and_predictor = []\n",
        "    # loop through all predictors that have yet to be added to the model and evaluate adding each one\n",
        "    for predictor in remaining_predictors:\n",
        "      # fit a logistic regression model with all previous predictors plus the new one\n",
        "      model = sm.Logit(y_train, sm.add_constant(X_train[selected_predictors + [predictor]])).fit();\n",
        "      # get the AIC value that corresponds to the added predictor and add to a list\n",
        "      aic_and_predictor.append((model.aic, predictor))\n",
        "\n",
        "    # now for that size model, sort all added predictors by AIC\n",
        "    aic_and_predictor.sort()\n",
        "    # get the predictor that has the lowest AIC\n",
        "    new_best_aic, new_best_predictor = aic_and_predictor[0]\n",
        "\n",
        "    # if the new best model that we just found improves the AIC, use it\n",
        "    if new_best_aic < current_best_aic:\n",
        "      # remove selected predictor from remaining predictors\n",
        "      remaining_predictors.remove(new_best_predictor)\n",
        "      # add new best predictor to list of selected variables\n",
        "      selected_predictors.append(new_best_predictor)\n",
        "      # update current best aic with new best aic\n",
        "      current_best_aic = new_best_aic\n",
        "    # no improvement\n",
        "    else:\n",
        "      # stop forward selection - model did not improve\n",
        "      break\n",
        "\n",
        "  return selected_predictors\n",
        "\n",
        "# run forward stepwise selection\n",
        "selected_predictors = forward_stepwise_logit_AIC(X_train, y_train)\n",
        "# print selected predictors\n",
        "print(\"Selected Predictors:\", selected_predictors)\n",
        "print(len(selected_predictors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57oVcpqO8JWG"
      },
      "source": [
        "Fit a logistic regression model with the selected predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhaenXf4831V"
      },
      "outputs": [],
      "source": [
        "# fit logistic regression model with the predictors chosen from forward stepwise using AIC\n",
        "# create design matrix\n",
        "X_train_design = sm.add_constant(X_train[selected_predictors])\n",
        "# fit logistic regression model\n",
        "model = sm.Logit(y_train, X_train_design).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN5RtdQn9o8j"
      },
      "source": [
        "Test the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2YJGeYP89HS"
      },
      "outputs": [],
      "source": [
        "# create design matrix for test set\n",
        "X_test_design = sm.add_constant(X_test[selected_predictors])\n",
        "# get predicted probabilities from the model\n",
        "predicted_probs = model.predict(X_test_design)\n",
        "# convert probabilities to predicted classes\n",
        "predicted_classes = (predicted_probs > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8SSITt-YYo"
      },
      "source": [
        "Evaluate the model using accuracy, precision, recall, and F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4UgSPTF-eq_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_classes))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, predicted_classes))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, predicted_classes))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, predicted_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nNaMT3F_584"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, predicted_classes)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Logistic Regression Model Confusion Matrix - AIC')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u02SuFteDOBD"
      },
      "source": [
        "3. Use BIC as criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PXlJzv0UDbDr"
      },
      "outputs": [],
      "source": [
        "# write a function that performs forward stepwise logistic regression using BIC criterion\n",
        "def forward_stepwise_logit_BIC(X_train, y_train):\n",
        "  # keep track of predictor variables that are still available to be added to the model\n",
        "  remaining_predictors = list(X_train.columns)\n",
        "  # list of selected predictor variables for the model\n",
        "  selected_predictors = []\n",
        "  # create variables for criterion that we want to minimize - initially set to infinity\n",
        "  current_best_bic, new_best_bic = float('inf'), float('inf')\n",
        "\n",
        "  # loop (at max) until there are no more predictors left to be added\n",
        "  while remaining_predictors:\n",
        "\n",
        "    bic_and_predictor = []\n",
        "    # loop through all predictors that have yet to be added to the model and evaluate adding each one\n",
        "    for predictor in remaining_predictors:\n",
        "      # fit a logistic regression model with all previous predictors plus the new one\n",
        "      model = sm.Logit(y_train, sm.add_constant(X_train[selected_predictors + [predictor]])).fit();\n",
        "      # get the BIC value that corresponds to the added predictor and add to a list\n",
        "      bic_and_predictor.append((model.bic, predictor))\n",
        "\n",
        "    # now for that size model, sort all added predictors by BIC\n",
        "    bic_and_predictor.sort()\n",
        "    # get the predictor that has the lowest BIC\n",
        "    new_best_bic, new_best_predictor = bic_and_predictor[0]\n",
        "\n",
        "    # if the new best model that we just found improves the BIC, use it\n",
        "    if new_best_bic < current_best_bic:\n",
        "      # remove selected predictor from remaining predictors\n",
        "      remaining_predictors.remove(new_best_predictor)\n",
        "      # add new best predictor to list of selected variables\n",
        "      selected_predictors.append(new_best_predictor)\n",
        "      # update current best bic with new best bic\n",
        "      current_best_bic = new_best_bic\n",
        "    # no improvement\n",
        "    else:\n",
        "      # stop forward selection - model did not improve\n",
        "      break\n",
        "\n",
        "  return selected_predictors\n",
        "\n",
        "# run forward stepwise selection\n",
        "selected_predictors = forward_stepwise_logit_BIC(X_train, y_train)\n",
        "# print selected predictors\n",
        "print(\"Selected Predictors:\", selected_predictors)\n",
        "print(len(selected_predictors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvdmsfkXD-RE"
      },
      "source": [
        "Fit a logistic regression model with the selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbZMmn_ID1wm"
      },
      "outputs": [],
      "source": [
        "# fit logistic regression model with the predictors chosen from forward stepwise using AIC\n",
        "# create design matrix\n",
        "X_train_design = sm.add_constant(X_train[selected_predictors])\n",
        "# fit logistic regression model\n",
        "model = sm.Logit(y_train, X_train_design).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btESePn-D-3s"
      },
      "source": [
        "Test the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yvOqghXEB4D"
      },
      "outputs": [],
      "source": [
        "# create design matrix for test set\n",
        "X_test_design = sm.add_constant(X_test[selected_predictors])\n",
        "# get predicted probabilities from the model\n",
        "predicted_probs = model.predict(X_test_design)\n",
        "# convert probabilities to predicted classes\n",
        "predicted_classes = (predicted_probs > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZlpkCWdEEE9"
      },
      "source": [
        "Evaluate the model using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmo9r1FnEGM6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_classes))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, predicted_classes))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, predicted_classes))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, predicted_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3TF7GmZEIVF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, predicted_classes)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Logistic Regression Model Confusion Matrix - BIC')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r1bcLTgnCxv"
      },
      "source": [
        "***Decision Tree***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f9jFeCwcgq"
      },
      "source": [
        "1. All features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2z7NihpmN8"
      },
      "source": [
        "Tune hyperparameters - criterion, max_depth, min_samples_split, min_samples_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8sYV1p_psxt"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# define a parameter grid that will be used to tune the hyperparameters\n",
        "param_grid_rt = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# create an instance of the DecisionTreeClassifier class that will be used to evaluate different hyperparameter combinations\n",
        "dt_class = DecisionTreeClassifier(random_state=42) # set random_state=42 for reproducibility\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation - trains k different models and uses k-1 folds each time as training data and the rest as validation\n",
        "grid_search_dt = GridSearchCV(estimator=dt_class, param_grid=param_grid_rt, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
        "# fit model on training data\n",
        "grid_search_dt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ8CVPjzrnVF"
      },
      "source": [
        "Get best estimator, parameters, and score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RueGSyW2rpD_"
      },
      "outputs": [],
      "source": [
        "# get best estimator after grid search\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "# get best parameters after grid search\n",
        "best_params = grid_search_dt.best_params_\n",
        "# get best score after grid search\n",
        "best_score = grid_search_dt.best_score_\n",
        "\n",
        "# print best parameters and best score\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4_PThtDsYWn"
      },
      "source": [
        "Predict on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PpCNrdpsZHz"
      },
      "outputs": [],
      "source": [
        "# predict on the test data\n",
        "dt_predictions = best_dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6JDZI4sfo_"
      },
      "source": [
        "Evaluate the model using accuracy, precision, recall, and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPfhYb5HsjUr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, dt_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, dt_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, dt_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, dt_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ck7cU6jss80"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, dt_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Decision Tree Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kNcoDsj_dD3"
      },
      "source": [
        "2. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJEgIn8V1e_o"
      },
      "source": [
        "Resplit data to get a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtctSzyu1eTF"
      },
      "outputs": [],
      "source": [
        "# split into training and test sets\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(predictors,\n",
        "                                                                     booking_status,\n",
        "                                                                     test_size=0.3,\n",
        "                                                                     random_state=0)\n",
        "# further split test set into validation and test set (so in terms of whole dataset, 15% test and 15% validation)\n",
        "# want a validation set to ensure that we don't use the test set in choosing the model - would be part of training\n",
        "X_val_tree, X_test_tree, y_val_tree, y_test_tree = train_test_split(X_test_tree,\n",
        "                                                                y_test_tree,\n",
        "                                                                test_size=0.5,\n",
        "                                                                random_state=0)\n",
        "\n",
        "print(X_train_tree.shape)\n",
        "print(X_val_tree.shape)\n",
        "print(X_test_tree.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDE0yiWAwfds"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# fit basic model without any tuning\n",
        "dt_class = DecisionTreeClassifier(random_state=42)\n",
        "dt_class.fit(X_train_tree, y_train_tree)\n",
        "# get most important features - use already fit model\n",
        "important_features = dt_class.feature_importances_\n",
        "# define a range of thresholds to test for best one\n",
        "thresholds = (0.01, 0.11, 0.01)\n",
        "\n",
        "# store results of cross validation - trying to find best threshold to use\n",
        "cv_results = {}\n",
        "\n",
        "# loop through each threshold and evaluate model with cross-validation\n",
        "for threshold in thresholds:\n",
        "  # selected features based on current threshold\n",
        "  selected_features = X_train.columns[important_features > threshold]\n",
        "  # extract selected features from training and testing sets\n",
        "  X_train_selected = X_train_tree[selected_features]\n",
        "  X_val_selected = X_val_tree[selected_features]\n",
        "\n",
        "  # perform cross-validation using selected features\n",
        "  scores = cross_val_score(dt_class, X_train_selected, y_train_tree, cv=5, scoring='f1') # using F1-Score since imblanaced classes\n",
        "\n",
        "  # store average F1 for the threshold\n",
        "  cv_results[threshold] = np.mean(scores)\n",
        "\n",
        "# find best threshold based on average cross validation performance\n",
        "best_threshold = max(cv_results, key=cv_results.get)\n",
        "\n",
        "print(\"Best Threshold:\", best_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPTHlmAG2Y8M"
      },
      "source": [
        "Fit model using best threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gurAyHhl2Xyh"
      },
      "outputs": [],
      "source": [
        "threshold = 0.01\n",
        "# get selected features based on best threshold\n",
        "selected_features = X_train_tree.columns[important_features > threshold]\n",
        "\n",
        "# use only selected features for training and test set\n",
        "X_train_selected = X_train_tree[selected_features]\n",
        "X_test_selected = X_test_tree[selected_features]\n",
        "\n",
        "# look at selected features\n",
        "selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIw4-cQ72xAl"
      },
      "source": [
        "Train model using only selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnq_nco32ytb"
      },
      "outputs": [],
      "source": [
        "# fit model on selected features\n",
        "dt_class.fit(X_train_selected, y_train_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxsx-_PuxXek"
      },
      "source": [
        "Tune hyperparameters for the selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7QnZba3xU0Q"
      },
      "outputs": [],
      "source": [
        "# define a parameter grid that will be used to tune the hyperparameters\n",
        "param_grid_rt = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation - trains k different models and uses k-1 folds each time as training data and the rest as validation\n",
        "grid_search_dt = GridSearchCV(estimator=dt_class, param_grid=param_grid_rt, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
        "# fit model on training data\n",
        "grid_search_dt.fit(X_train_selected, y_train_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbksbiVWxoQR"
      },
      "source": [
        "Get best estimators, parameters, and score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0rJix51xjSz"
      },
      "outputs": [],
      "source": [
        "# get best estimator after grid search\n",
        "best_dt_selected = grid_search_dt.best_estimator_\n",
        "# get best parameters after grid search\n",
        "best_params_selected = grid_search_dt.best_params_\n",
        "# get best score after grid search\n",
        "best_score_selected = grid_search_dt.best_score_\n",
        "\n",
        "# print best parameters and best score\n",
        "print(\"Best Parameters:\", best_params_selected)\n",
        "print(\"Best Score:\", best_score_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYwPTvD8xp4r"
      },
      "source": [
        "Predict on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6468Q6AWxsrm"
      },
      "outputs": [],
      "source": [
        "# predict on the test data\n",
        "dt_predictions_selected = dt_class.predict(X_test_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-jZMuoRxugw"
      },
      "source": [
        "Evaluate the selected features model using accuracy, precision, recall, F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaZs_UHAxx6_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tree, dt_predictions_selected))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test_tree, dt_predictions_selected))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test_tree, dt_predictions_selected))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test_tree, dt_predictions_selected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbdoUEFJx4m4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test_tree, dt_predictions_selected)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Feature Selection - Decision Tree Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7KvNA0runXO"
      },
      "source": [
        "***Random Forest***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzULJ9O8_jSl"
      },
      "source": [
        "1. All Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgLvR10wvozQ"
      },
      "source": [
        "Tune hyperparameters - n_estimators, max_features, max_depth, max_leaf_nodes, max_sample, min_sample_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00vrGtoZusHX"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# define a parameter grid that will be used to tune the hyperparameters\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [5, 10, 20, 30, None],\n",
        "    'max_leaf_nodes': [3, 6, 9, None]\n",
        "}\n",
        "\n",
        "# create an instance of the RandomForestClassifier() that will be used to evaluate different hyperparameter combinations\n",
        "rf_class = RandomForestClassifier(random_state=42) # set random_state for reproducibility\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation\n",
        "grid_search_rf = GridSearchCV(estimator=rf_class, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# fit model on training data\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# get best estimator after grid search\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "# print best estimator\n",
        "print(best_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ODIBHqrzQT5"
      },
      "source": [
        "Fit model with chosen hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7EXXzyyzMnt"
      },
      "outputs": [],
      "source": [
        "best_rf_model = RandomForestClassifier(n_estimators=150)\n",
        "# fit best model on the training data\n",
        "best_rf_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGcjk2iKzvQk"
      },
      "source": [
        "Predict on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9amBoXHzwHP"
      },
      "outputs": [],
      "source": [
        "# make predictions on our test data\n",
        "rf_predictions = best_rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIvyX2k1z1-i"
      },
      "source": [
        "Evaluate model using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMvHsfTVz5H1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, rf_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, rf_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, rf_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXOrMsj-z8wY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, rf_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Random Forest Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySkCAYe1_lY7"
      },
      "source": [
        "2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeMY5bTcAVWg"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# fit basic model without any tuning\n",
        "rf_class = RandomForestClassifier(random_state=42)\n",
        "rf_class.fit(X_train_tree, y_train_tree)\n",
        "# get most important features - use already fit model\n",
        "important_features = rf_class.feature_importances_\n",
        "# define a range of thresholds to test for best one\n",
        "thresholds = (0.01, 0.11, 0.01)\n",
        "\n",
        "# store results of cross validation - trying to find best threshold to use\n",
        "cv_results = {}\n",
        "\n",
        "# loop through each threshold and evaluate model with cross-validation\n",
        "for threshold in thresholds:\n",
        "  # selected features based on current threshold\n",
        "  selected_features = X_train.columns[important_features > threshold]\n",
        "  # extract selected features from training and testing sets\n",
        "  X_train_selected = X_train_tree[selected_features]\n",
        "  X_val_selected = X_val_tree[selected_features]\n",
        "\n",
        "  # perform cross-validation using selected features\n",
        "  scores = cross_val_score(rf_class, X_train_selected, y_train_tree, cv=5, scoring='f1') # using F1-Score since imblanaced classes\n",
        "\n",
        "  # store average F1 for the threshold\n",
        "  cv_results[threshold] = np.mean(scores)\n",
        "\n",
        "# find best threshold based on average cross validation performance\n",
        "best_threshold = max(cv_results, key=cv_results.get)\n",
        "\n",
        "print(\"Best Threshold:\", best_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI0yOSPbAi2C"
      },
      "source": [
        "Select features based on best threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkZDdS1uAlsF"
      },
      "outputs": [],
      "source": [
        "threshold = 0.01\n",
        "# get selected features based on best threshold\n",
        "selected_features = X_train_tree.columns[important_features > threshold]\n",
        "\n",
        "# use only selected features for training and test set\n",
        "X_train_selected = X_train_tree[selected_features]\n",
        "X_test_selected = X_test_tree[selected_features]\n",
        "\n",
        "# look at selected features\n",
        "selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc9FH-kqA-EP"
      },
      "source": [
        "Fit model on selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm9223XsA-0N"
      },
      "outputs": [],
      "source": [
        "# fit model on selected features\n",
        "rf_class.fit(X_train_selected, y_train_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilDyrjqABE8G"
      },
      "source": [
        "Tune hyperparameters for the selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t6-l6kRBGZA"
      },
      "outputs": [],
      "source": [
        "# define a parameter grid that will be used to tune the hyperparameters\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [5, 10, 20, 30, None],\n",
        "    'max_leaf_nodes': [3, 6, 9, None]\n",
        "    #'max_samples': [0.5, 0.7, 0.8, 1.0],\n",
        "    #'min_samples_split': [2, 5, 10, 20, 50, 100]\n",
        "}\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation\n",
        "grid_search_rf = GridSearchCV(estimator=rf_class, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# fit model on training data\n",
        "grid_search_rf.fit(X_train_selected, y_train_tree)\n",
        "\n",
        "# get best estimator after grid search\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "# print best estimator\n",
        "print(best_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mME8zZHKBZjk"
      },
      "source": [
        "Predict on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QEKen-tBarc"
      },
      "outputs": [],
      "source": [
        "# make predictions on our test data\n",
        "rf_predictions = best_rf.predict(X_test_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCqXkXlABpCw"
      },
      "source": [
        "Evaluate model using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iFwyk6CBrD5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tree, rf_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test_tree, rf_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test_tree, rf_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test_tree, rf_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwvSN53TBvm0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test_tree, rf_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Feature Selection - Random Forest Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVINoW0cIuyU"
      },
      "source": [
        "### Implement additional (non-baseline) models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xboMMu0ADj62"
      },
      "source": [
        "1. K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5nBLTvW4tGR"
      },
      "source": [
        "a. All Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtmiBnurLKR7"
      },
      "source": [
        "Tune hyperparameters - n_neighbors, weights, metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03dbA47iLAyu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# create instance of KNeighborsClassifier\n",
        "knn_class = KNeighborsClassifier()\n",
        "# define parameter grid that will be used to tune hyperparameters\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation\n",
        "grid_search_knn = GridSearchCV(estimator=knn_class, param_grid=param_grid_knn, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# fit model on training data\n",
        "grid_search_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# get best estimator after grid search\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "# print best estimator\n",
        "print(best_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG_n69ofMGew"
      },
      "source": [
        "Get best estimator, parameters, and score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vlmEHafMFEr"
      },
      "outputs": [],
      "source": [
        "# get best estimator after grid search\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "# get best parameters after grid search\n",
        "best_params_knn = grid_search_knn.best_params_\n",
        "# get best score after grid search\n",
        "best_score_knn = grid_search_knn.best_score_\n",
        "\n",
        "# print best parameters and best score\n",
        "print(\"Best Parameters:\", best_params_knn)\n",
        "print(\"Best Score:\", best_score_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqd0iWzMV11"
      },
      "source": [
        "Predict on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdkssrEmMW0l"
      },
      "outputs": [],
      "source": [
        "# make predictions on our test data\n",
        "knn_predictions = best_knn.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89BRksp3Mc8S"
      },
      "source": [
        "Evaluate performance of model on test data using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oVy-j0LMiBg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, knn_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, knn_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, knn_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, knn_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TBj2zJsMko_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, knn_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('KNN Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pemf0Wq14xmi"
      },
      "source": [
        "b. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ19RqqV6iAk"
      },
      "source": [
        "Resplit code to have validation set for feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ydUdwde5Fyv"
      },
      "outputs": [],
      "source": [
        "# split into training and test sets\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(predictors,\n",
        "                                                                     booking_status,\n",
        "                                                                     test_size=0.3,\n",
        "                                                                     random_state=0)\n",
        "# further split test set into validation and test set (so in terms of whole dataset, 15% test and 15% validation)\n",
        "# want a validation set to ensure that we don't use the test set in choosing the model - would be part of training\n",
        "X_val_knn, X_test_knn, y_val_knn, y_test_knn = train_test_split(X_test_knn,\n",
        "                                                                y_test_knn,\n",
        "                                                                test_size=0.5,\n",
        "                                                                random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rra9ay3Z739n"
      },
      "source": [
        "Rescale data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6a4Ry0h74Zv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on X_train and scale the features\n",
        "scaled_X_train = scaler.fit_transform(X_train_knn)\n",
        "\n",
        "# Scale X_test using the same scaler\n",
        "scaled_X_test = scaler.transform(X_test_knn)\n",
        "\n",
        "# Scale X_val using the same scaler\n",
        "scaled_X_val = scaler.transform(X_val_knn)\n",
        "\n",
        "# Convert back to DataFrames with the original columns and reset indices\n",
        "X_train_scaled = pd.DataFrame(scaled_X_train, columns=X_train_knn.columns, index=X_train_knn.index)\n",
        "X_test_scaled = pd.DataFrame(scaled_X_test, columns=X_test_knn.columns, index=X_test_knn.index)\n",
        "X_val_scaled = pd.DataFrame(scaled_X_val, columns=X_val_knn.columns, index=X_val_knn.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lppn-3X6k22"
      },
      "source": [
        "Feature selection - choose which predictors to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkQK9spd5QqU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# define variables that will keep track of the best model so far, the k-value (number of predictors), and it's accuracy/performance\n",
        "best_k = None\n",
        "best_accuracy = 0\n",
        "best_model_knn = None\n",
        "best_X_train_selected = None\n",
        "best_X_test_selected = None\n",
        "\n",
        "# loop through all possible number of predictors (1-17)\n",
        "for k in range(1, 17):\n",
        "  # create an instance of SelectKBest\n",
        "  selector = SelectKBest(chi2, k=k) # use chi-square as statistical test used for feature selection\n",
        "  # apply the selector to the training data\n",
        "  X_train_selected = selector.fit_transform(X_train_scaled, y_train_knn)\n",
        "  # apply the same feature selection to the validation data\n",
        "  X_val_selected = selector.transform(X_val_scaled)\n",
        "  X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "  # define an instance of the KNearestNeighbors classifier\n",
        "  model = KNeighborsClassifier()\n",
        "  # fit the model on the selected training data\n",
        "  model.fit(X_train_selected, y_train_knn)\n",
        "\n",
        "  # predict on the validation set\n",
        "  knn_predictions = model.predict(X_val_selected)\n",
        "\n",
        "  # compute the accuracy of the model on the validation set\n",
        "  current_accuracy = accuracy_score(y_val_knn, knn_predictions)\n",
        "\n",
        "  # check if the new accuracy is better than the best saved\n",
        "  if current_accuracy > best_accuracy:\n",
        "    # update the best accuracy to be the current accuracy\n",
        "    best_accuracy = current_accuracy\n",
        "    # update the best k value - number of predictors\n",
        "    best_k = k\n",
        "    # update the best KNN model\n",
        "    best_model_knn = model\n",
        "    # update the best predictors\n",
        "    best_X_train_selected = X_train_selected\n",
        "    best_X_test_selected = X_test_selected\n",
        "\n",
        "# print out number of features used\n",
        "print(\"Number of Features (k)\", best_k)\n",
        "best_model_knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFAuzMV68ek"
      },
      "source": [
        "Tune hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSClPr6D69Nd"
      },
      "outputs": [],
      "source": [
        "# define parameter grid that will be used to tune hyperparameters\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# use GridSearchCV to perform the grid search with cross-validation\n",
        "grid_search_knn = GridSearchCV(estimator=best_model_knn, param_grid=param_grid_knn, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# fit model on training data\n",
        "grid_search_knn.fit(best_X_train_selected, y_train_knn)\n",
        "\n",
        "# get best estimator after grid search\n",
        "best_model_knn = grid_search_knn.best_estimator_\n",
        "# print best estimator\n",
        "print(best_model_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A691zbe7Itq"
      },
      "source": [
        "Get best estimator, parameters, and score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhWemPsV7JzL"
      },
      "outputs": [],
      "source": [
        "# get best parameters after grid search\n",
        "best_params_knn = grid_search_knn.best_params_\n",
        "# get best score after grid search\n",
        "best_score_knn = grid_search_knn.best_score_\n",
        "\n",
        "# print best parameters and best score\n",
        "print(\"Best Parameters:\", best_params_knn)\n",
        "print(\"Best Score:\", best_score_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZeRa_6B7PR9"
      },
      "source": [
        "Predict on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmdIH5ma7ixF"
      },
      "outputs": [],
      "source": [
        "# make predictions on our test data\n",
        "knn_predictions = best_model_knn.predict(best_X_test_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUw4xSiV9ORU"
      },
      "source": [
        "Evaluate performance of model using accuracy, precision, recall, and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc7wGc7I9RXf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_knn, knn_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test_knn, knn_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test_knn, knn_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test_knn, knn_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWxZHquI9XFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test_knn, knn_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Feature Selection - KNN Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeIpxgpiDmBe"
      },
      "source": [
        "2. Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXW_S7socFIT"
      },
      "source": [
        "1. All Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGzFwvPWcEAP"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# define an instance of the model\n",
        "model = GaussianNB()\n",
        "# fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on the test set\n",
        "gnb_predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2p_0YPAczE4"
      },
      "source": [
        "Evaluate performance of model using accuracy, precision, recall, F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwF9_XPEc1-d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, gnb_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test, gnb_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test, gnb_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test, gnb_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8puswyUc7oI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test, gnb_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Gaussian Naive Bayes Model Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPgG9_fXcGPr"
      },
      "source": [
        "2. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiUipYojIJde"
      },
      "source": [
        "Resplit data into training, validation, and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw4AiK7fIItC"
      },
      "outputs": [],
      "source": [
        "# split into training and test sets\n",
        "X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(predictors,\n",
        "                                                                     booking_status,\n",
        "                                                                     test_size=0.3,\n",
        "                                                                     random_state=0)\n",
        "# further split test set into validation and test set (so in terms of whole dataset, 15% test and 15% validation)\n",
        "# want a validation set to ensure that we don't use the test set in choosing the model - would be part of training\n",
        "X_val_gnb, X_test_gnb, y_val_gnb, y_test_gnb = train_test_split(X_test_gnb,\n",
        "                                                                y_test_gnb,\n",
        "                                                                test_size=0.5,\n",
        "                                                                random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igJJNRAAGk88"
      },
      "source": [
        "Feature selection - choose predictors to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXz43OE4Go3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# define variables that will keep track of the best model so far, the k-value (number of predictors), and it's accuracy/performance\n",
        "best_k = None\n",
        "best_accuracy = 0\n",
        "best_model_gnb = None\n",
        "best_X_train_selected = None\n",
        "best_X_test_selected = None\n",
        "\n",
        "# loop through all possible number of predictors (1-17)\n",
        "for k in range(1, 17):\n",
        "  # create an instance of SelectKBest\n",
        "  selector = SelectKBest(chi2, k=k) # use chi-square as statistical test used for feature selection\n",
        "  # apply the selector to the training data\n",
        "  X_train_selected = selector.fit_transform(X_train_gnb, y_train_gnb)\n",
        "  # apply the same feature selection to the validation data\n",
        "  X_val_selected = selector.transform(X_val_gnb)\n",
        "  X_test_selected = selector.transform(X_test_gnb)\n",
        "\n",
        "  # define an instance of the Gaussian Naive Bayes classifier\n",
        "  model = GaussianNB()\n",
        "  # fit the model on the selected training data\n",
        "  model.fit(X_train_selected, y_train_gnb)\n",
        "\n",
        "  # predict on the validation set\n",
        "  gnb_predictions = model.predict(X_val_selected)\n",
        "\n",
        "  # compute the accuracy of the model on the validation set\n",
        "  current_accuracy = accuracy_score(y_val_gnb, gnb_predictions)\n",
        "\n",
        "  # check if the new accuracy is better than the best saved\n",
        "  if current_accuracy > best_accuracy:\n",
        "    # update the best accuracy to be the current accuracy\n",
        "    best_accuracy = current_accuracy\n",
        "    # update the best k value - number of predictors\n",
        "    best_k = k\n",
        "    # update the best NB model\n",
        "    best_model_gnb = model\n",
        "    # update the best predictors\n",
        "    best_X_train_selected = X_train_selected\n",
        "    best_X_test_selected = X_test_selected\n",
        "\n",
        "# print out number of features used\n",
        "print(\"Number of Features (k)\", best_k)\n",
        "best_model_gnb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZkYzfeiFDvW"
      },
      "source": [
        "Predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uodn4B4jFEb5"
      },
      "outputs": [],
      "source": [
        "gnb_predictions = best_model_gnb.predict(best_X_test_selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoy4NiJFIQq"
      },
      "source": [
        "Evaluate performance of model using accuracy, precision, recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0JzNYv1FLp3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# calculate and print accuracy of model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_gnb, gnb_predictions))\n",
        "# calculate and print precision of model\n",
        "print(\"Precision:\", precision_score(y_test_gnb, gnb_predictions))\n",
        "# calculate and print recall of model\n",
        "print(\"Recall:\", recall_score(y_test_gnb, gnb_predictions))\n",
        "# calculate and print f1_score of model\n",
        "print(\"F1 Score:\", f1_score(y_test_gnb, gnb_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-z29aj_FPlc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create confusion matrix of results\n",
        "cm = confusion_matrix(y_test_gnb, gnb_predictions)\n",
        "# create heatmap using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Feature Selection - Gaussian Naive Bayes Model Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}